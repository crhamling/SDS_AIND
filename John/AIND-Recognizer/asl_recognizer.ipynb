{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Artificial Intelligence Engineer Nanodegree - Probabilistic Models\n",
    "## Project: Sign Language Recognition System\n",
    "- [Introduction](#intro)\n",
    "- [Part 1 Feature Selection](#part1_tutorial)\n",
    "    - [Tutorial](#part1_tutorial)\n",
    "    - [Features Submission](#part1_submission)\n",
    "    - [Features Unittest](#part1_test)\n",
    "- [Part 2 Train the models](#part2_tutorial)\n",
    "    - [Tutorial](#part2_tutorial)\n",
    "    - [Model Selection Score Submission](#part2_submission)\n",
    "    - [Model Score Unittest](#part2_test)\n",
    "- [Part 3 Build a Recognizer](#part3_tutorial)\n",
    "    - [Tutorial](#part3_tutorial)\n",
    "    - [Recognizer Submission](#part3_submission)\n",
    "    - [Recognizer Unittest](#part3_test)\n",
    "- [Part 4 (OPTIONAL) Improve the WER with Language Models](#part4_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "The overall goal of this project is to build a word recognizer for American Sign Language video sequences, demonstrating the power of probabalistic models.  In particular, this project employs  [hidden Markov models (HMM's)](https://en.wikipedia.org/wiki/Hidden_Markov_model) to analyze a series of measurements taken from videos of American Sign Language (ASL) collected for research (see the [RWTH-BOSTON-104 Database](http://www-i6.informatik.rwth-aachen.de/~dreuw/database-rwth-boston-104.php)).  In this video, the right-hand x and y locations are plotted as the speaker signs the sentence.\n",
    "[![ASLR demo](http://www-i6.informatik.rwth-aachen.de/~dreuw/images/demosample.png)](https://drive.google.com/open?id=0B_5qGuFe-wbhUXRuVnNZVnMtam8)\n",
    "\n",
    "The raw data, train, and test sets are pre-defined.  You will derive a variety of feature sets (explored in Part 1), as well as implement three different model selection criterion to determine the optimal number of hidden states for each word model (explored in Part 2). Finally, in Part 3 you will implement the recognizer and compare the effects the different combinations of feature sets and model selection criteria.  \n",
    "\n",
    "At the end of each Part, complete the submission cells with implementations, answer all questions, and pass the unit tests.  Then submit the completed notebook for review!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part1_tutorial'></a>\n",
    "## PART 1: Data\n",
    "\n",
    "### Features Tutorial\n",
    "##### Load the initial database\n",
    "A data handler designed for this database is provided in the student codebase as the `AslDb` class in the `asl_data` module.  This handler creates the initial [pandas](http://pandas.pydata.org/pandas-docs/stable/) dataframe from the corpus of data included in the `data` directory as well as dictionaries suitable for extracting data in a format friendly to the [hmmlearn](https://hmmlearn.readthedocs.io/en/latest/) library.  We'll use those to create models in Part 2.\n",
    "\n",
    "To start, let's set up the initial database and select an example set of features for the training set.  At the end of Part 1, you will create additional feature sets for experimentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>left-x</th>\n",
       "      <th>left-y</th>\n",
       "      <th>right-x</th>\n",
       "      <th>right-y</th>\n",
       "      <th>nose-x</th>\n",
       "      <th>nose-y</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">98</th>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             left-x  left-y  right-x  right-y  nose-x  nose-y  speaker\n",
       "video frame                                                           \n",
       "98    0         149     181      170      175     161      62  woman-1\n",
       "      1         149     181      170      175     161      62  woman-1\n",
       "      2         149     181      170      175     161      62  woman-1\n",
       "      3         149     181      170      175     161      62  woman-1\n",
       "      4         149     181      170      175     161      62  woman-1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from asl_data import AslDb\n",
    "\n",
    "\n",
    "asl = AslDb() # initializes the database\n",
    "asl.df.head() # displays the first five rows of the asl database, indexed by video and frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "left-x         149\n",
       "left-y         181\n",
       "right-x        170\n",
       "right-y        175\n",
       "nose-x         161\n",
       "nose-y          62\n",
       "speaker    woman-1\n",
       "Name: (98, 1), dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asl.df.ix[98,1]  # look at the data available for an individual frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The frame represented by video 98, frame 1 is shown here:\n",
    "![Video 98](http://www-i6.informatik.rwth-aachen.de/~dreuw/database/rwth-boston-104/overview/images/orig/098-start.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Feature selection for training the model\n",
    "The objective of feature selection when training a model is to choose the most relevant variables while keeping the model as simple as possible, thus reducing training time.  We can use the raw features already provided or derive our own and add columns to the pandas dataframe `asl.df` for selection. As an example, in the next cell a feature named `'grnd-ry'` is added. This feature is the difference between the right-hand y value and the nose y value, which serves as the \"ground\" right y value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>left-x</th>\n",
       "      <th>left-y</th>\n",
       "      <th>right-x</th>\n",
       "      <th>right-y</th>\n",
       "      <th>nose-x</th>\n",
       "      <th>nose-y</th>\n",
       "      <th>speaker</th>\n",
       "      <th>grnd-ry</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">98</th>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             left-x  left-y  right-x  right-y  nose-x  nose-y  speaker  \\\n",
       "video frame                                                              \n",
       "98    0         149     181      170      175     161      62  woman-1   \n",
       "      1         149     181      170      175     161      62  woman-1   \n",
       "      2         149     181      170      175     161      62  woman-1   \n",
       "      3         149     181      170      175     161      62  woman-1   \n",
       "      4         149     181      170      175     161      62  woman-1   \n",
       "\n",
       "             grnd-ry  \n",
       "video frame           \n",
       "98    0          113  \n",
       "      1          113  \n",
       "      2          113  \n",
       "      3          113  \n",
       "      4          113  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asl.df['grnd-ry'] = asl.df['right-y'] - asl.df['nose-y']\n",
    "asl.df.head()  # the new feature 'grnd-ry' is now in the frames dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asl.df sample\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>left-x</th>\n",
       "      <th>left-y</th>\n",
       "      <th>right-x</th>\n",
       "      <th>right-y</th>\n",
       "      <th>nose-x</th>\n",
       "      <th>nose-y</th>\n",
       "      <th>speaker</th>\n",
       "      <th>grnd-ry</th>\n",
       "      <th>grnd-rx</th>\n",
       "      <th>grnd-ly</th>\n",
       "      <th>grnd-lx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">98</th>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             left-x  left-y  right-x  right-y  nose-x  nose-y  speaker  \\\n",
       "video frame                                                              \n",
       "98    0         149     181      170      175     161      62  woman-1   \n",
       "      1         149     181      170      175     161      62  woman-1   \n",
       "      2         149     181      170      175     161      62  woman-1   \n",
       "      3         149     181      170      175     161      62  woman-1   \n",
       "      4         149     181      170      175     161      62  woman-1   \n",
       "\n",
       "             grnd-ry  grnd-rx  grnd-ly  grnd-lx  \n",
       "video frame                                      \n",
       "98    0          113        9      119      -12  \n",
       "      1          113        9      119      -12  \n",
       "      2          113        9      119      -12  \n",
       "      3          113        9      119      -12  \n",
       "      4          113        9      119      -12  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<font color=green>Correct!</font><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from asl_utils import test_features_tryit\n",
    "# Add df columns for 'grnd-rx', 'grnd-ly', 'grnd-lx' representing differences between hand and nose locations\n",
    "asl.df['grnd-rx'] = asl.df['right-x'] - asl.df['nose-x']\n",
    "asl.df['grnd-ly'] = asl.df['left-y'] - asl.df['nose-y']\n",
    "asl.df['grnd-lx'] = asl.df['left-x'] - asl.df['nose-x']\n",
    "\n",
    "# test the code\n",
    "test_features_tryit(asl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 113, -12, 119]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect the features into a list\n",
    "features_ground = ['grnd-rx','grnd-ry','grnd-lx','grnd-ly']\n",
    " #show a single set of features for a given (video, frame) tuple\n",
    "[asl.df.ix[98,1][v] for v in features_ground]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Build the training set\n",
    "Now that we have a feature list defined, we can pass that list to the `build_training` method to collect the features for all the words in the training set.  Each word in the training set has multiple examples from various videos.  Below we can see the unique words that have been loaded into the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training words: ['JOHN', 'WRITE', 'HOMEWORK', 'IX-1P', 'SEE', 'YESTERDAY', 'IX', 'LOVE', 'MARY', 'CAN', 'GO', 'GO1', 'FUTURE', 'GO2', 'PARTY', 'FUTURE1', 'HIT', 'BLAME', 'FRED', 'FISH', 'WONT', 'EAT', 'BUT', 'CHICKEN', 'VEGETABLE', 'CHINA', 'PEOPLE', 'PREFER', 'BROCCOLI', 'LIKE', 'LEAVE', 'SAY', 'BUY', 'HOUSE', 'KNOW', 'CORN', 'CORN1', 'THINK', 'NOT', 'PAST', 'LIVE', 'CHICAGO', 'CAR', 'SHOULD', 'DECIDE', 'VISIT', 'MOVIE', 'WANT', 'SELL', 'TOMORROW', 'NEXT-WEEK', 'NEW-YORK', 'LAST-WEEK', 'WILL', 'FINISH', 'ANN', 'READ', 'BOOK', 'CHOCOLATE', 'FIND', 'SOMETHING-ONE', 'POSS', 'BROTHER', 'ARRIVE', 'HERE', 'GIVE', 'MAN', 'NEW', 'COAT', 'WOMAN', 'GIVE1', 'HAVE', 'FRANK', 'BREAK-DOWN', 'SEARCH-FOR', 'WHO', 'WHAT', 'LEG', 'FRIEND', 'CANDY', 'BLUE', 'SUE', 'BUY1', 'STOLEN', 'OLD', 'STUDENT', 'VIDEOTAPE', 'BORROW', 'MOTHER', 'POTATO', 'TELL', 'BILL', 'THROW', 'APPLE', 'NAME', 'SHOOT', 'SAY-1P', 'SELF', 'GROUP', 'JANA', 'TOY1', 'MANY', 'TOY', 'ALL', 'BOY', 'TEACHER', 'GIRL', 'BOX', 'GIVE2', 'GIVE3', 'GET', 'PUTASIDE']\n"
     ]
    }
   ],
   "source": [
    "training = asl.build_training(features_ground)\n",
    "print(\"Training words: {}\".format(training.words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The training data in `training` is an object of class `WordsData` defined in the `asl_data` module.  in addition to the `words` list, data can be accessed with the `get_all_sequences`, `get_all_Xlengths`, `get_word_sequences`, and `get_word_Xlengths` methods. We need the `get_word_Xlengths` method to train multiple sequences with the `hmmlearn` library.  In the following example, notice that there are two lists; the first is a concatenation of all the sequences(the X portion) and the second is a list of the sequence lengths(the Lengths portion)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-11,  48,   7, 120],\n",
       "        [-11,  48,   8, 109],\n",
       "        [ -8,  49,  11,  98],\n",
       "        [ -7,  50,   7,  87],\n",
       "        [ -4,  54,   7,  77],\n",
       "        [ -4,  54,   6,  69],\n",
       "        [ -4,  54,   6,  69],\n",
       "        [-13,  52,   6,  69],\n",
       "        [-13,  52,   6,  69],\n",
       "        [ -8,  51,   6,  69],\n",
       "        [ -8,  51,   6,  69],\n",
       "        [ -8,  51,   6,  69],\n",
       "        [ -8,  51,   6,  69],\n",
       "        [ -8,  51,   6,  69],\n",
       "        [-10,  59,   7,  71],\n",
       "        [-15,  64,   9,  77],\n",
       "        [-17,  75,  13,  81],\n",
       "        [ -4,  48,  -4, 113],\n",
       "        [ -2,  53,  -4, 113],\n",
       "        [ -4,  55,   2,  98],\n",
       "        [ -4,  58,   2,  98],\n",
       "        [ -1,  59,   2,  89],\n",
       "        [ -1,  59,  -1,  84],\n",
       "        [ -1,  59,  -1,  84],\n",
       "        [ -7,  63,  -1,  84],\n",
       "        [ -7,  63,  -1,  84],\n",
       "        [ -7,  63,   3,  83],\n",
       "        [ -7,  63,   3,  83],\n",
       "        [ -7,  63,   3,  83],\n",
       "        [ -7,  63,   3,  83],\n",
       "        [ -7,  63,   3,  83],\n",
       "        [ -7,  63,   3,  83],\n",
       "        [ -7,  63,   3,  83],\n",
       "        [ -4,  70,   3,  83],\n",
       "        [ -4,  70,   3,  83],\n",
       "        [ -2,  73,   5,  90],\n",
       "        [ -3,  79,  -4,  96],\n",
       "        [-15,  98,  13, 135],\n",
       "        [ -6,  93,  12, 128],\n",
       "        [ -2,  89,  14, 118],\n",
       "        [  5,  90,  10, 108],\n",
       "        [  4,  86,   7, 105],\n",
       "        [  4,  86,   7, 105],\n",
       "        [  4,  86,  13, 100],\n",
       "        [ -3,  82,  14,  96],\n",
       "        [ -3,  82,  14,  96],\n",
       "        [  6,  89,  16, 100],\n",
       "        [  6,  89,  16, 100],\n",
       "        [  7,  85,  17, 111]], dtype=int64), [17, 20, 12])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.get_word_Xlengths('CHOCOLATE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###### More feature sets\n",
    "So far we have a simple feature set that is enough to get started modeling.  However, we might get better results if we manipulate the raw values a bit more, so we will go ahead and set up some other options now for experimentation later.  For example, we could normalize each speaker's range of motion with grouped statistics using [Pandas stats](http://pandas.pydata.org/pandas-docs/stable/api.html#api-dataframe-stats) functions and [pandas groupby](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html).  Below is an example for finding the means of all speaker subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left-x</th>\n",
       "      <th>left-y</th>\n",
       "      <th>right-x</th>\n",
       "      <th>right-y</th>\n",
       "      <th>nose-x</th>\n",
       "      <th>nose-y</th>\n",
       "      <th>grnd-ry</th>\n",
       "      <th>grnd-rx</th>\n",
       "      <th>grnd-ly</th>\n",
       "      <th>grnd-lx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>man-1</th>\n",
       "      <td>206.248203</td>\n",
       "      <td>218.679449</td>\n",
       "      <td>155.464350</td>\n",
       "      <td>150.371031</td>\n",
       "      <td>175.031756</td>\n",
       "      <td>61.642600</td>\n",
       "      <td>88.728430</td>\n",
       "      <td>-19.567406</td>\n",
       "      <td>157.036848</td>\n",
       "      <td>31.216447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman-1</th>\n",
       "      <td>164.661438</td>\n",
       "      <td>161.271242</td>\n",
       "      <td>151.017865</td>\n",
       "      <td>117.332462</td>\n",
       "      <td>162.655120</td>\n",
       "      <td>57.245098</td>\n",
       "      <td>60.087364</td>\n",
       "      <td>-11.637255</td>\n",
       "      <td>104.026144</td>\n",
       "      <td>2.006318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman-2</th>\n",
       "      <td>183.214509</td>\n",
       "      <td>176.527232</td>\n",
       "      <td>156.866295</td>\n",
       "      <td>119.835714</td>\n",
       "      <td>170.318973</td>\n",
       "      <td>58.022098</td>\n",
       "      <td>61.813616</td>\n",
       "      <td>-13.452679</td>\n",
       "      <td>118.505134</td>\n",
       "      <td>12.895536</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             left-x      left-y     right-x     right-y      nose-x  \\\n",
       "speaker                                                               \n",
       "man-1    206.248203  218.679449  155.464350  150.371031  175.031756   \n",
       "woman-1  164.661438  161.271242  151.017865  117.332462  162.655120   \n",
       "woman-2  183.214509  176.527232  156.866295  119.835714  170.318973   \n",
       "\n",
       "            nose-y    grnd-ry    grnd-rx     grnd-ly    grnd-lx  \n",
       "speaker                                                          \n",
       "man-1    61.642600  88.728430 -19.567406  157.036848  31.216447  \n",
       "woman-1  57.245098  60.087364 -11.637255  104.026144   2.006318  \n",
       "woman-2  58.022098  61.813616 -13.452679  118.505134  12.895536  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means = asl.df.groupby('speaker').mean()\n",
    "df_means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To select a mean that matches by speaker, use the pandas [map](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>left-x</th>\n",
       "      <th>left-y</th>\n",
       "      <th>right-x</th>\n",
       "      <th>right-y</th>\n",
       "      <th>nose-x</th>\n",
       "      <th>nose-y</th>\n",
       "      <th>speaker</th>\n",
       "      <th>grnd-ry</th>\n",
       "      <th>grnd-rx</th>\n",
       "      <th>grnd-ly</th>\n",
       "      <th>grnd-lx</th>\n",
       "      <th>left-x-mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <th>frame</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">98</th>\n",
       "      <th>0</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "      <td>164.661438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "      <td>164.661438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "      <td>164.661438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "      <td>164.661438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>149</td>\n",
       "      <td>181</td>\n",
       "      <td>170</td>\n",
       "      <td>175</td>\n",
       "      <td>161</td>\n",
       "      <td>62</td>\n",
       "      <td>woman-1</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>119</td>\n",
       "      <td>-12</td>\n",
       "      <td>164.661438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             left-x  left-y  right-x  right-y  nose-x  nose-y  speaker  \\\n",
       "video frame                                                              \n",
       "98    0         149     181      170      175     161      62  woman-1   \n",
       "      1         149     181      170      175     161      62  woman-1   \n",
       "      2         149     181      170      175     161      62  woman-1   \n",
       "      3         149     181      170      175     161      62  woman-1   \n",
       "      4         149     181      170      175     161      62  woman-1   \n",
       "\n",
       "             grnd-ry  grnd-rx  grnd-ly  grnd-lx  left-x-mean  \n",
       "video frame                                                   \n",
       "98    0          113        9      119      -12   164.661438  \n",
       "      1          113        9      119      -12   164.661438  \n",
       "      2          113        9      119      -12   164.661438  \n",
       "      3          113        9      119      -12   164.661438  \n",
       "      4          113        9      119      -12   164.661438  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asl.df['left-x-mean']= asl.df['speaker'].map(df_means['left-x'])\n",
    "asl.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_std\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>left-x</th>\n",
       "      <th>left-y</th>\n",
       "      <th>right-x</th>\n",
       "      <th>right-y</th>\n",
       "      <th>nose-x</th>\n",
       "      <th>nose-y</th>\n",
       "      <th>grnd-ry</th>\n",
       "      <th>grnd-rx</th>\n",
       "      <th>grnd-ly</th>\n",
       "      <th>grnd-lx</th>\n",
       "      <th>left-x-mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>man-1</th>\n",
       "      <td>15.154425</td>\n",
       "      <td>36.328485</td>\n",
       "      <td>18.901917</td>\n",
       "      <td>54.902340</td>\n",
       "      <td>6.654573</td>\n",
       "      <td>5.520045</td>\n",
       "      <td>53.487999</td>\n",
       "      <td>20.269032</td>\n",
       "      <td>36.572749</td>\n",
       "      <td>15.080360</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman-1</th>\n",
       "      <td>17.573442</td>\n",
       "      <td>26.594521</td>\n",
       "      <td>16.459943</td>\n",
       "      <td>34.667787</td>\n",
       "      <td>3.549392</td>\n",
       "      <td>3.538330</td>\n",
       "      <td>33.972660</td>\n",
       "      <td>16.764706</td>\n",
       "      <td>27.117393</td>\n",
       "      <td>17.328941</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>woman-2</th>\n",
       "      <td>15.388711</td>\n",
       "      <td>28.825025</td>\n",
       "      <td>14.890288</td>\n",
       "      <td>39.649111</td>\n",
       "      <td>4.099760</td>\n",
       "      <td>3.416167</td>\n",
       "      <td>39.128572</td>\n",
       "      <td>16.191324</td>\n",
       "      <td>29.320655</td>\n",
       "      <td>15.050938</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            left-x     left-y    right-x    right-y    nose-x    nose-y  \\\n",
       "speaker                                                                   \n",
       "man-1    15.154425  36.328485  18.901917  54.902340  6.654573  5.520045   \n",
       "woman-1  17.573442  26.594521  16.459943  34.667787  3.549392  3.538330   \n",
       "woman-2  15.388711  28.825025  14.890288  39.649111  4.099760  3.416167   \n",
       "\n",
       "           grnd-ry    grnd-rx    grnd-ly    grnd-lx  left-x-mean  \n",
       "speaker                                                           \n",
       "man-1    53.487999  20.269032  36.572749  15.080360          0.0  \n",
       "woman-1  33.972660  16.764706  27.117393  17.328941          0.0  \n",
       "woman-2  39.128572  16.191324  29.320655  15.050938          0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<font color=green>Correct!</font><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from asl_utils import test_std_tryit\n",
    "# Create a dataframe named `df_std` with standard deviations grouped by speaker\n",
    "df_std = asl.df.groupby('speaker').std()\n",
    "\n",
    "# test the code\n",
    "test_std_tryit(df_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part1_submission'></a>\n",
    "### Features Implementation Submission\n",
    "Implement four feature sets and answer the question that follows.\n",
    "- normalized Cartesian coordinates\n",
    "    - use *mean* and *standard deviation* statistics and the [standard score](https://en.wikipedia.org/wiki/Standard_score) equation to account for speakers with different heights and arm length\n",
    "    \n",
    "- polar coordinates\n",
    "    - calculate polar coordinates with [Cartesian to polar equations](https://en.wikipedia.org/wiki/Polar_coordinate_system#Converting_between_polar_and_Cartesian_coordinates)\n",
    "    - use the [np.arctan2](https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.arctan2.html) function and *swap the x and y axes* to move the $0$ to $2\\pi$ discontinuity to 12 o'clock instead of 3 o'clock;  in other words, the normal break in radians value from $0$ to $2\\pi$ occurs directly to the left of the speaker's nose, which may be in the signing area and interfere with results.  By swapping the x and y axes, that discontinuity move to directly above the speaker's head, an area not generally used in signing.\n",
    "\n",
    "- delta difference\n",
    "    - as described in Thad's lecture, use the difference in values between one frame and the next frames as features\n",
    "    - pandas [diff method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.diff.html) and [fillna method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html) will be helpful for this one\n",
    "\n",
    "- custom features\n",
    "    - These are your own design; combine techniques used above or come up with something else entirely. We look forward to seeing what you come up with! \n",
    "    Some ideas to get you started:\n",
    "        - normalize using a [feature scaling equation](https://en.wikipedia.org/wiki/Feature_scaling)\n",
    "        - normalize the polar coordinates\n",
    "        - adding additional deltas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add features for normalized by speaker values of left, right, x, y\n",
    "\n",
    "features_norm = ['norm-rx', 'norm-ry', 'norm-lx','norm-ly']\n",
    "\n",
    "mean_right_x = asl.df['speaker'].map(df_means['right-x'])\n",
    "mean_right_y = asl.df['speaker'].map(df_means['right-y'])\n",
    "mean_left_x = asl.df['speaker'].map(df_means['left-x'])\n",
    "mean_left_y = asl.df['speaker'].map(df_means['left-y'])\n",
    "\n",
    "std_right_x = asl.df['speaker'].map(df_std['right-x'])\n",
    "std_right_y = asl.df['speaker'].map(df_std['right-y'])\n",
    "std_left_x  = asl.df['speaker'].map(df_std['left-x'])\n",
    "std_left_y  = asl.df['speaker'].map(df_std['left-y'])\n",
    "\n",
    "# Name these 'norm-rx', 'norm-ry', 'norm-lx', and 'norm-ly'\n",
    "# using Z-score scaling (X-Xmean)/Xstd\n",
    "asl.df['norm-rx'] = (asl.df['right-x'] - mean_right_x) / std_right_x\n",
    "asl.df['norm-ry'] = (asl.df['right-y'] - mean_right_y) / std_right_y\n",
    "asl.df['norm-lx'] = (asl.df['left-x'] - mean_left_x) / std_left_x\n",
    "asl.df['norm-ly'] = (asl.df['left-y'] - mean_left_y) / std_left_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add features for polar coordinate values where the nose is the origin (Ground)\n",
    "# Name these 'polar-rr', 'polar-rtheta', 'polar-lr', and 'polar-ltheta'\n",
    "# Note that 'polar-rr' and 'polar-rtheta' refer to the radius and angle\n",
    "\n",
    "features_polar = ['polar-rr', 'polar-rtheta', 'polar-lr', 'polar-ltheta']\n",
    "\n",
    "asl.df['polar-rr'] = np.sqrt(asl.df['grnd-rx']*asl.df['grnd-rx'] + asl.df['grnd-ry']*asl.df['grnd-ry'])\n",
    "asl.df['polar-rtheta'] = np.arctan2(asl.df['grnd-rx'], asl.df['grnd-ry'])\n",
    "asl.df['polar-lr'] = np.sqrt(asl.df['grnd-lx']*asl.df['grnd-lx'] + asl.df['grnd-ly']*asl.df['grnd-ly'])\n",
    "asl.df['polar-ltheta'] = np.arctan2(asl.df['grnd-lx'], asl.df['grnd-ly'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Add features for left, right, x, y differences by one time step, i.e. the \"delta\" values discussed in the lecture\n",
    "# Name these 'delta-rx', 'delta-ry', 'delta-lx', and 'delta-ly'\n",
    "\n",
    "features_delta = ['delta-rx', 'delta-ry', 'delta-lx', 'delta-ly']\n",
    "\n",
    "asl.df['delta-rx'] = asl.df['right-x'].diff().fillna(0)\n",
    "asl.df['delta-ry'] = asl.df['right-y'].diff().fillna(0)\n",
    "asl.df['delta-lx'] = asl.df['left-x'].diff().fillna(0)\n",
    "asl.df['delta-ly'] = asl.df['left-y'].diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Define a list named 'features_custom' for building the training set\n",
    "\n",
    "features_custom = ['win3-mean-rx', 'win3-mean-ry', 'win3-mean-lx', 'win3-mean-ly', 'win3-mean-nx', 'win3-mean-ny']\n",
    "\n",
    "# Add features of your own design, which may be a combination of the above or something else\n",
    "# Name these whatever you would like\n",
    "\n",
    "asl.df['win3-mean-rx'] = asl.df['right-x'].groupby(level='video').rolling(3).mean().fillna(method='bfill').values\n",
    "asl.df['win3-mean-ry'] = asl.df['right-y'].groupby(level='video').rolling(3).mean().fillna(method='bfill').values\n",
    "asl.df['win3-mean-lx'] = asl.df['left-x'].groupby(level='video').rolling(3).mean().fillna(method='bfill').values\n",
    "asl.df['win3-mean-ly'] = asl.df['left-y'].groupby(level='video').rolling(3).mean().fillna(method='bfill').values\n",
    "asl.df['win3-mean-nx'] = asl.df['nose-x'].groupby(level='video').rolling(3).mean().fillna(method='bfill').values\n",
    "asl.df['win3-mean-ny'] = asl.df['nose-y'].groupby(level='video').rolling(3).mean().fillna(method='bfill').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 1:**  What custom features did you choose for the features_custom set and why?\n",
    "\n",
    "**Answer 1:** The features are rolling windows of 3 frames that average each of the raw input values for both hands and the nose over this window. The goal of these features is to give the model insight into the history of the raw input coordinates and distinguish the difference between when a hand or nose moves vs when it is stationary over multiple frames at any given point in time. \n",
    "\n",
    "This hypothesis ends up being false. The performance of these features is terrible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part1_test'></a>\n",
    "### Features Unit Testing\n",
    "Run the following unit tests as a sanity check on the defined \"ground\", \"norm\", \"polar\", and 'delta\"\n",
    "feature sets.  The test simply looks for some valid values but is not exhaustive.  However, the project should not be submitted if these tests don't pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.028s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "# import numpy as np\n",
    "\n",
    "class TestFeatures(unittest.TestCase):\n",
    "\n",
    "    def test_features_ground(self):\n",
    "        sample = (asl.df.ix[98, 1][features_ground]).tolist()\n",
    "        self.assertEqual(sample, [9, 113, -12, 119])\n",
    "\n",
    "    def test_features_norm(self):\n",
    "        sample = (asl.df.ix[98, 1][features_norm]).tolist()\n",
    "        np.testing.assert_almost_equal(sample, [ 1.153,  1.663, -0.891,  0.742], 3)\n",
    "\n",
    "    def test_features_polar(self):\n",
    "        sample = (asl.df.ix[98,1][features_polar]).tolist()\n",
    "        np.testing.assert_almost_equal(sample, [113.3578, 0.0794, 119.603, -0.1005], 3)\n",
    "\n",
    "    def test_features_delta(self):\n",
    "        sample = (asl.df.ix[98, 0][features_delta]).tolist()\n",
    "        self.assertEqual(sample, [0, 0, 0, 0])\n",
    "        sample = (asl.df.ix[98, 18][features_delta]).tolist()\n",
    "        self.assertTrue(sample in [[-16, -5, -2, 4], [-14, -9, 0, 0]], \"Sample value found was {}\".format(sample))\n",
    "                         \n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestFeatures())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part2_tutorial'></a>\n",
    "## PART 2: Model Selection\n",
    "### Model Selection Tutorial\n",
    "The objective of Model Selection is to tune the number of states for each word HMM prior to testing on unseen data.  In this section you will explore three methods: \n",
    "- Log likelihood using cross-validation folds (CV)\n",
    "- Bayesian Information Criterion (BIC)\n",
    "- Discriminative Information Criterion (DIC) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Train a single word\n",
    "Now that we have built a training set with sequence data, we can \"train\" models for each word.  As a simple starting example, we train a single word using Gaussian hidden Markov models (HMM).   By using the `fit` method during training, the [Baum-Welch Expectation-Maximization](https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm) (EM) algorithm is invoked iteratively to find the best estimate for the model *for the number of hidden states specified* from a group of sample seequences. For this example, we *assume* the correct number of hidden states is 3, but that is just a guess.  How do we know what the \"best\" number of states for training is?  We will need to find some model selection technique to choose the best parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states trained in model for BOOK is 3\n",
      "logL = -2331.113812743319\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "def train_a_word(word, num_hidden_states, features):\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    training = asl.build_training(features)  \n",
    "    X, lengths = training.get_word_Xlengths(word)\n",
    "    model = GaussianHMM(n_components=num_hidden_states, n_iter=1000).fit(X, lengths)\n",
    "    logL = model.score(X, lengths)\n",
    "    return model, logL\n",
    "\n",
    "demoword = 'BOOK'\n",
    "model, logL = train_a_word(demoword, 3, features_ground)\n",
    "print(\"Number of states trained in model for {} is {}\".format(demoword, model.n_components))\n",
    "print(\"logL = {}\".format(logL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "The HMM model has been trained and information can be pulled from the model, including means and variances for each feature and hidden state.  The [log likelihood](http://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution) for any individual sample or group of samples can also be calculated with the `score` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states trained in model for BOOK is 3\n",
      "hidden state #0\n",
      "mean =  [ -3.46504869  50.66686933  14.02391587  52.04731066]\n",
      "variance =  [ 49.12346305  43.04799144  39.35109609  47.24195772]\n",
      "\n",
      "hidden state #1\n",
      "mean =  [ -11.45300909   94.109178     19.03512475  102.2030162 ]\n",
      "variance =  [  77.403668    203.35441965   26.68898447  156.12444034]\n",
      "\n",
      "hidden state #2\n",
      "mean =  [ -1.12415027  69.44164191  17.02866283  77.7231196 ]\n",
      "variance =  [ 19.70434594  16.83041492  30.51552305  11.03678246]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_model_stats(word, model):\n",
    "    print(\"Number of states trained in model for {} is {}\".format(word, model.n_components))    \n",
    "    variance=np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])    \n",
    "    for i in range(model.n_components):  # for each hidden state\n",
    "        print(\"hidden state #{}\".format(i))\n",
    "        print(\"mean = \", model.means_[i])\n",
    "        print(\"variance = \", variance[i])\n",
    "        print()\n",
    "    \n",
    "show_model_stats(demoword, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Try it!\n",
    "Experiment by changing the feature set, word, and/or num_hidden_states values in the next cell to see changes in values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states trained in model for TOMORROW is 5\n",
      "hidden state #0\n",
      "mean =  [ -7.20000000e+00   2.54000000e+01   6.40000000e+00   1.00800000e+02\n",
      "   4.12038789e-01  -1.03647983e+00   3.83451468e-01  -1.68126430e-01\n",
      "   2.65211956e+01  -2.62447921e-01   1.01056381e+02   6.54106587e-02\n",
      "   2.00000000e+00  -2.80000000e+00  -2.00000000e+00   7.00000000e+00\n",
      "   1.62000000e+02   2.37333333e+02   2.03000000e+02   2.38000000e+02\n",
      "   1.78000000e+02   5.90000000e+01]\n",
      "variance =  [  1.21620000e+01   2.98420000e+01   9.04200000e+00   8.53620000e+01\n",
      "   4.68824865e-02   2.68282770e-02   3.12721600e-02   1.22689662e-01\n",
      "   3.56281851e+01   1.33234373e-02   8.36099351e+01   3.00294654e-03\n",
      "   3.60200000e+00   1.49620000e+01   1.08020000e+01   4.80200000e+00\n",
      "   2.00000000e-03   1.77977778e+00   2.00000000e-03   2.00000000e-03\n",
      "   2.00000000e-03   2.00000000e-03]\n",
      "\n",
      "hidden state #1\n",
      "mean =  [ -6.66666667e+00   3.73333333e+01  -4.00000000e+00   1.12000000e+02\n",
      "   5.05194234e-01  -5.48034075e-01  -1.51446593e-01   4.41021603e-01\n",
      "   3.79504935e+01  -1.79378800e-01   1.12071406e+02  -3.56991127e-02\n",
      "  -1.00000000e+00   4.66666667e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.51000000e+02   1.37333333e+02   2.13777778e+02   2.04666667e+02\n",
      "   1.74333333e+02   5.58888889e+01]\n",
      "variance =  [  8.92222222e-01   1.15588889e+01   3.33333333e-03   3.33333333e-03\n",
      "   6.61421685e-03   1.29480965e-02   3.33333333e-03   3.33333333e-03\n",
      "   1.04300445e+01   4.76049828e-03   3.33333333e-03   3.33333333e-03\n",
      "   8.67000000e+00   3.55888889e+00   3.33333333e-03   3.33333333e-03\n",
      "   1.78551852e+01   5.21851852e-01   3.65765432e+00   9.91885185e+01\n",
      "   9.66296296e-01   2.80246914e-02]\n",
      "\n",
      "hidden state #2\n",
      "mean =  [ -3.33333333e+00   2.76666667e+01  -3.66666667e+00   1.13666667e+02\n",
      "   6.87454887e-01  -8.74946588e-01  -1.51446593e-01   4.41021603e-01\n",
      "   2.78759534e+01  -1.21686529e-01   1.13727089e+02  -3.23445446e-02\n",
      "  -3.33333333e-01   4.33333333e+00  -1.33333333e+00   1.33333333e+00\n",
      "   1.59666667e+02   2.11888889e+02   2.03000000e+02   2.38000000e+02\n",
      "   1.78111111e+02   5.86666667e+01]\n",
      "variance =  [  2.25555556e-01   4.22555556e+00   2.25555556e-01   5.55888889e+00\n",
      "   4.15355421e-03   4.99742696e-03   3.33333333e-03   3.33333333e-03\n",
      "   3.93455265e+00   4.02500074e-03   5.48590663e+00   3.35583959e-03\n",
      "   1.55888889e+00   1.75588889e+01   3.55888889e+00   3.55888889e+00\n",
      "   1.55888889e+00   1.60472469e+02   3.33333333e-03   3.33333333e-03\n",
      "   2.80246914e-02   2.25555556e-01]\n",
      "\n",
      "hidden state #3\n",
      "mean =  [ -1.10000000e+01   3.40000000e+01   1.10000000e+01   7.80000000e+01\n",
      "   1.81175295e-01  -7.88410922e-01   6.45210093e-01  -1.02544588e+00\n",
      "   3.57351368e+01  -3.12901228e-01   7.87718224e+01   1.40101724e-01\n",
      "  -7.00000000e+00  -5.00000000e+00  -4.00000000e+00   3.00000000e+00\n",
      "   1.65000000e+02   1.75000000e+02   1.46000000e+02   1.84000000e+02\n",
      "   1.58333333e+02   6.03333333e+01]\n",
      "variance =  [ 0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01\n",
      "  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01]\n",
      "\n",
      "hidden state #4\n",
      "mean =  [ -3.00000000e+00   2.80000000e+01  -4.00000000e+00   1.12000000e+02\n",
      "   7.27957254e-01  -8.17256144e-01  -1.51446593e-01   4.41021603e-01\n",
      "   2.81602557e+01  -1.06735673e-01   1.12071406e+02  -3.56991127e-02\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.49400000e+02   1.56333333e+02   2.06066667e+02   2.33533333e+02\n",
      "   1.78333333e+02   5.62000000e+01]\n",
      "variance =  [  2.00000000e-03   2.00000000e-03   2.00000000e-03   2.00000000e-03\n",
      "   2.00000000e-03   2.00000000e-03   2.00000000e-03   2.00000000e-03\n",
      "   2.00000000e-03   2.00000000e-03   2.00000000e-03   2.00000000e-03\n",
      "   2.00000000e-03   2.00000000e-03   2.00000000e-03   2.00000000e-03\n",
      "   1.21975556e+01   2.11113111e+02   5.61977778e+00   1.59842222e+01\n",
      "   5.79777778e-01   1.62000000e-01]\n",
      "\n",
      "logL = 13.77645398859001\n"
     ]
    }
   ],
   "source": [
    "my_testword = 'TOMORROW'\n",
    "model, logL = train_a_word(my_testword, 5, np.concatenate((features_ground, features_norm, features_polar, features_delta, features_custom), axis=0)) # Experiment here with different parameters\n",
    "show_model_stats(my_testword, model)\n",
    "print(\"logL = {}\".format(logL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Visualize the hidden states\n",
    "We can plot the means and variances for each state and feature.  Try varying the number of states trained for the HMM model and examine the variances.  Are there some models that are \"better\" than others?  How can you tell?  We would like to hear what you think in the classroom online."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: %1 is not a valid Win32 application.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e27d371d6baa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2158\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2079\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2080\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-105>\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\IPython\\core\\magics\\pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m             \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[1;34m(self, gui)\u001b[0m\n\u001b[0;32m   2947\u001b[0m                 \u001b[0mgui\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2949\u001b[1;33m         \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivate_matplotlib\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2950\u001b[0m         \u001b[0mpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfigure_inline_support\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\IPython\\core\\pylabtools.py\u001b[0m in \u001b[0;36mactivate_matplotlib\u001b[1;34m(backend)\u001b[0m\n\u001b[0;32m    305\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'backend'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 307\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m     \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pylab_helpers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontour\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcontour\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\contour.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcoll\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont_manager\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfont_manager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmlab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\text.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mallow_rasterization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend_bases\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRendererBase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_bbox\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtight_bbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextpath\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtextpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmplDeprecation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_deprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\textpath.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mft2font\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKERNING_DEFAULT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOAD_NO_HINTING\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mft2font\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLOAD_TARGET_LIGHT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmathtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMathTextParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdviread\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdviread\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont_manager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFontProperties\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_font\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\mathtext.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_png\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_png\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m####################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: %1 is not a valid Win32 application."
     ]
    }
   ],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: %1 is not a valid Win32 application.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-0cd3471b782d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"\"\" visualize the input model for a particular word \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcycler\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcycler\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstyle\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_pylab_helpers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\colorbar.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontour\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcontour\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgridspec\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\contour.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollections\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcoll\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont_manager\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfont_manager\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmlab\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmlab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\text.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0martist\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mallow_rasterization\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend_bases\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRendererBase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\backend_bases.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_bbox\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtight_bbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextpath\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtextpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcbook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmplDeprecation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarn_deprecated\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\textpath.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mft2font\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKERNING_DEFAULT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLOAD_NO_HINTING\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mft2font\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLOAD_TARGET_LIGHT\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmathtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMathTextParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdviread\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdviread\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfont_manager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFontProperties\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_font\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\aind\\lib\\site-packages\\matplotlib\\mathtext.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolors\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_png\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_png\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;31m####################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed: %1 is not a valid Win32 application."
     ]
    }
   ],
   "source": [
    "import math\n",
    "from matplotlib import (cm, pyplot as plt, mlab)\n",
    "\n",
    "def visualize(word, model):\n",
    "    \"\"\" visualize the input model for a particular word \"\"\"\n",
    "    variance=np.array([np.diag(model.covars_[i]) for i in range(model.n_components)])\n",
    "    figures = []\n",
    "    for parm_idx in range(len(model.means_[0])):\n",
    "        xmin = int(min(model.means_[:,parm_idx]) - max(variance[:,parm_idx]))\n",
    "        xmax = int(max(model.means_[:,parm_idx]) + max(variance[:,parm_idx]))\n",
    "        fig, axs = plt.subplots(model.n_components, sharex=True, sharey=False)\n",
    "        colours = cm.rainbow(np.linspace(0, 1, model.n_components))\n",
    "        for i, (ax, colour) in enumerate(zip(axs, colours)):\n",
    "            x = np.linspace(xmin, xmax, 100)\n",
    "            mu = model.means_[i,parm_idx]\n",
    "            sigma = math.sqrt(np.diag(model.covars_[i])[parm_idx])\n",
    "            ax.plot(x, mlab.normpdf(x, mu, sigma), c=colour)\n",
    "            ax.set_title(\"{} feature {} hidden state #{}\".format(word, parm_idx, i))\n",
    "\n",
    "            ax.grid(True)\n",
    "        figures.append(plt)\n",
    "    for p in figures:\n",
    "        p.show()\n",
    "        \n",
    "visualize(my_testword, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#####  ModelSelector class\n",
    "Review the `ModelSelector` class from the codebase found in the `my_model_selectors.py` module.  It is designed to be a strategy pattern for choosing different model selectors.  For the project submission in this section, subclass `SelectorModel` to implement the following model selectors.  In other words, you will write your own classes/functions in the `my_model_selectors.py` module and run them from this notebook:\n",
    "\n",
    "- `SelectorCV `:  Log likelihood with CV\n",
    "- `SelectorBIC`: BIC \n",
    "- `SelectorDIC`: DIC\n",
    "\n",
    "You will train each word in the training set with a range of values for the number of hidden states, and then score these alternatives with the model selector, choosing the \"best\" according to each strategy. The simple case of training with a constant value for `n_components` can be called using the provided `SelectorConstant` subclass as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states trained in model for VEGETABLE is 3\n"
     ]
    }
   ],
   "source": [
    "from my_model_selectors import SelectorConstant\n",
    "\n",
    "training = asl.build_training(features_ground)  # Experiment here with different feature sets defined in part 1\n",
    "word = 'VEGETABLE' # Experiment here with different words\n",
    "model = SelectorConstant(training.get_all_sequences(), training.get_all_Xlengths(), word, n_constant=3).select()\n",
    "print(\"Number of states trained in model for {} is {}\".format(word, model.n_components))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Cross-validation folds\n",
    "If we simply score the model with the Log Likelihood calculated from the feature sequences it has been trained on, we should expect that more complex models will have higher likelihoods. However, that doesn't tell us which would have a better likelihood score on unseen data.  The model will likely be overfit as complexity is added.  To estimate which topology model is better using only the training data, we can compare scores using cross-validation.  One technique for cross-validation is to break the training set into \"folds\" and rotate which fold is left out of training.  The \"left out\" fold scored.  This gives us a proxy method of finding the best model to use on \"unseen data\". In the following example, a set of word sequences is broken into three folds using the [scikit-learn Kfold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) class object. When you implement `SelectorCV`, you will use this technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train fold indices:[2 3 4 5] Test fold indices:[0 1]\n",
      "Train fold indices:[0 1 4 5] Test fold indices:[2 3]\n",
      "Train fold indices:[0 1 2 3] Test fold indices:[4 5]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "training = asl.build_training(features_ground) # Experiment here with different feature sets\n",
    "word = 'VEGETABLE' # Experiment here with different words\n",
    "word_sequences = training.get_word_sequences(word)\n",
    "split_method = KFold()\n",
    "for cv_train_idx, cv_test_idx in split_method.split(word_sequences):\n",
    "    print(\"Train fold indices:{} Test fold indices:{}\".format(cv_train_idx, cv_test_idx))  # view indices of the folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Tip:** In order to run `hmmlearn` training using the X,lengths tuples on the new folds, subsets must be combined based on the indices given for the folds.  A helper utility has been provided in the `asl_utils` module named `combine_sequences` for this purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Scoring models with other criterion\n",
    "Scoring model topologies with **BIC** balances fit and complexity within the training set for each word.  In the BIC equation, a penalty term penalizes complexity to avoid overfitting, so that it is not necessary to also use cross-validation in the selection process.  There are a number of references on the internet for this criterion.  These [slides](http://www2.imm.dtu.dk/courses/02433/doc/ch6_slides.pdf) include a formula you may find helpful for your implementation.\n",
    "\n",
    "The advantages of scoring model topologies with **DIC** over BIC are presented by Alain Biem in this [reference](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.58.6208&rep=rep1&type=pdf) (also found [here](https://pdfs.semanticscholar.org/ed3d/7c4a5f607201f3848d4c02dd9ba17c791fc2.pdf)).  DIC scores the discriminant ability of a training set for one word against competing words.  Instead of a penalty term for complexity, it provides a penalty if model liklihoods for non-matching words are too similar to model likelihoods for the correct word in the word set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part2_submission'></a>\n",
    "### Model Selection Implementation Submission\n",
    "Implement `SelectorCV`, `SelectorBIC`, and `SelectorDIC` classes in the `my_model_selectors.py` module.  Run the selectors on the following five words. Then answer the questions about your results.\n",
    "\n",
    "**Tip:** The `hmmlearn` library may not be able to train or score all models.  Implement try/except contructs as necessary to eliminate non-viable models from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "words_to_train = ['FISH', 'BOOK', 'VEGETABLE', 'FUTURE', 'JOHN']\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# autoreload for automatically reloading changes made in my_model_selectors and my_recognizer\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete for FISH with 3 states with time 0.032519904362672165 seconds\n",
      "Training complete for BOOK with 3 states with time 0.13901058739847014 seconds\n",
      "Training complete for VEGETABLE with 3 states with time 0.04535722541082729 seconds\n",
      "Training complete for FUTURE with 3 states with time 0.1305097890820298 seconds\n",
      "Training complete for JOHN with 3 states with time 0.9776150106695058 seconds\n"
     ]
    }
   ],
   "source": [
    "# Implement SelectorCV in my_model_selector.py\n",
    "from my_model_selectors import SelectorCV\n",
    "\n",
    "training = asl.build_training(features_ground)  # Experiment here with different feature sets defined in part 1\n",
    "sequences = training.get_all_sequences()\n",
    "Xlengths = training.get_all_Xlengths()\n",
    "for word in words_to_train:\n",
    "    start = timeit.default_timer()\n",
    "    model = SelectorCV(sequences, Xlengths, word, \n",
    "                    min_n_components=2, max_n_components=15, random_state = 14).select()\n",
    "    end = timeit.default_timer()-start\n",
    "    if model is not None:\n",
    "        print(\"Training complete for {} with {} states with time {} seconds\".format(word, model.n_components, end))\n",
    "    else:\n",
    "        print(\"Training failed for {}\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete for FISH with 2 states with time 0.1025812082824995 seconds\n",
      "Training complete for BOOK with 2 states with time 2.422260757191319 seconds\n",
      "Training complete for VEGETABLE with 2 states with time 0.8478825649295896 seconds\n",
      "Training complete for FUTURE with 2 states with time 2.5597983191177374 seconds\n",
      "Training complete for JOHN with 2 states with time 22.060555135597774 seconds\n"
     ]
    }
   ],
   "source": [
    "# Implement SelectorBIC in module my_model_selectors.py\n",
    "from my_model_selectors import SelectorBIC\n",
    "\n",
    "training = asl.build_training(features_ground)  # Experiment here with different feature sets defined in part 1\n",
    "sequences = training.get_all_sequences()\n",
    "Xlengths = training.get_all_Xlengths()\n",
    "for word in words_to_train:\n",
    "    start = timeit.default_timer()\n",
    "    model = SelectorBIC(sequences, Xlengths, word, \n",
    "                    min_n_components=2, max_n_components=15, random_state = 14).select()\n",
    "    end = timeit.default_timer()-start\n",
    "    if model is not None:\n",
    "        print(\"Training complete for {} with {} states with time {} seconds\".format(word, model.n_components, end))\n",
    "    else:\n",
    "        print(\"Training failed for {}\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete for FISH with 3 states with time 0.11068708762203983 seconds\n",
      "Training complete for BOOK with 3 states with time 2.2311824454275495 seconds\n",
      "Training complete for VEGETABLE with 3 states with time 0.8338444224499568 seconds\n",
      "Training complete for FUTURE with 3 states with time 2.3120440024525166 seconds\n",
      "Training complete for JOHN with 3 states with time 22.256548738356578 seconds\n"
     ]
    }
   ],
   "source": [
    "# Implement SelectorDIC in module my_model_selectors.py\n",
    "from my_model_selectors import SelectorDIC\n",
    "\n",
    "training = asl.build_training(features_ground)  # Experiment here with different feature sets defined in part 1\n",
    "sequences = training.get_all_sequences()\n",
    "Xlengths = training.get_all_Xlengths()\n",
    "for word in words_to_train:\n",
    "    start = timeit.default_timer()\n",
    "    model = SelectorDIC(sequences, Xlengths, word, \n",
    "                    min_n_components=2, max_n_components=15, random_state = 14).select()\n",
    "    end = timeit.default_timer()-start\n",
    "    if model is not None:\n",
    "        print(\"Training complete for {} with {} states with time {} seconds\".format(word, model.n_components, end))\n",
    "    else:\n",
    "        print(\"Training failed for {}\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 2:**  Compare and contrast the possible advantages and disadvantages of the various model selectors implemented.\n",
    "\n",
    "**Answer 2:** The advantage to using cross-fold validation is that you are able to train the model with less data, though, this does lead to some overhead in computing the data folds and subsetting the training set. On the other hand, BIC and DIC require more data since they are not using cross-folds to simulate a larger dataset. However, BIC includes model complexity as part of the optimization cost and therefore produces models with less parameters. DIC does not penalize complexity, but rather offers better performance than a comparable BIC model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part2_test'></a>\n",
    "### Model Selector Unit Testing\n",
    "Run the following unit tests as a sanity check on the implemented model selectors.  The test simply looks for valid interfaces  but is not exhaustive. However, the project should not be submitted if these tests don't pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 20.026s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=4 errors=0 failures=0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from asl_test_model_selectors import TestSelectors\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestSelectors())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part3_tutorial'></a>\n",
    "## PART 3: Recognizer\n",
    "The objective of this section is to \"put it all together\".  Using the four feature sets created and the three model selectors, you will experiment with the models and present your results.  Instead of training only five specific words as in the previous section, train the entire set with a feature set and model selector strategy.  \n",
    "### Recognizer Tutorial\n",
    "##### Train the full training set\n",
    "The following example trains the entire set with the example `features_ground` and `SelectorConstant` features and model selector.  Use this pattern for you experimentation and final submission cells.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word models returned = 112\n"
     ]
    }
   ],
   "source": [
    "from my_model_selectors import SelectorConstant\n",
    "\n",
    "def train_all_words(features, model_selector):\n",
    "    training = asl.build_training(features)  # Experiment here with different feature sets defined in part 1\n",
    "    sequences = training.get_all_sequences()\n",
    "    Xlengths = training.get_all_Xlengths()\n",
    "    model_dict = {}\n",
    "    for word in training.words:\n",
    "        model = model_selector(sequences, Xlengths, word, \n",
    "                        n_constant=3).select()\n",
    "        model_dict[word]=model\n",
    "    return model_dict\n",
    "\n",
    "models = train_all_words(features_ground, SelectorConstant)\n",
    "print(\"Number of word models returned = {}\".format(len(models)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "##### Load the test set\n",
    "The `build_test` method in `ASLdb` is similar to the `build_training` method already presented, but there are a few differences:\n",
    "- the object is type `SinglesData` \n",
    "- the internal dictionary keys are the index of the test word rather than the word itself\n",
    "- the getter methods are `get_all_sequences`, `get_all_Xlengths`, `get_item_sequences` and `get_item_Xlengths`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test set items: 178\n",
      "Number of test set sentences: 40\n"
     ]
    }
   ],
   "source": [
    "test_set = asl.build_test(features_ground)\n",
    "print(\"Number of test set items: {}\".format(test_set.num_items))\n",
    "print(\"Number of test set sentences: {}\".format(len(test_set.sentences_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part3_submission'></a>\n",
    "### Recognizer Implementation Submission\n",
    "For the final project submission, students must implement a recognizer following guidance in the `my_recognizer.py` module.  Experiment with the four feature sets and the three model selection methods (that's 12 possible combinations). You can add and remove cells for experimentation or run the recognizers locally in some other way during your experiments, but retain the results for your discussion.  For submission, you will provide code cells of **only three** interesting combinations for your discussion (see questions below). At least one of these should produce a word error rate of less than 60%, i.e. WER < 0.60 . \n",
    "\n",
    "**Tip:** The hmmlearn library may not be able to train or score all models.  Implement try/except contructs as necessary to eliminate non-viable models from consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Implement the recognize method in my_recognizer\n",
    "from my_recognizer import recognize\n",
    "from asl_utils import show_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** WER = 0.6179775280898876\n",
      "Total correct: 68 out of 178\n",
      "Video  Recognized                                                    Correct\n",
      "=====================================================================================================\n",
      "    2: *GO WRITE HOMEWORK                                            JOHN WRITE HOMEWORK\n",
      "    7: JOHN *WHAT *MARY *WHAT                                        JOHN CAN GO CAN\n",
      "   12: JOHN *WHAT *GO1 CAN                                           JOHN CAN GO CAN\n",
      "   21: *IX *HOMEWORK WONT *FUTURE *CAR *CAR *GO *TOMORROW            JOHN FISH WONT EAT BUT CAN EAT CHICKEN\n",
      "   25: *FRANK LIKE IX *WHO IX                                        JOHN LIKE IX IX IX\n",
      "   28: *IX *WHO *FUTURE *FUTURE IX                                   JOHN LIKE IX IX IX\n",
      "   30: *SHOULD LIKE *GO *MARY *GO                                    JOHN LIKE IX IX IX\n",
      "   36: *SOMETHING-ONE VEGETABLE *GIRL *GIVE *MARY *MARY              MARY VEGETABLE KNOW IX LIKE CORN1\n",
      "   40: *SUE *GIVE *DECIDE MARY *GO                                   JOHN IX THINK MARY LOVE\n",
      "   43: *IX *GO BUY HOUSE                                             JOHN MUST BUY HOUSE\n",
      "   50: *POSS *SEE BUY CAR *ARRIVE                                    FUTURE JOHN BUY CAR SHOULD\n",
      "   54: JOHN SHOULD *WHO BUY HOUSE                                    JOHN SHOULD NOT BUY HOUSE\n",
      "   57: *MARY *PREFER *MARY MARY                                      JOHN DECIDE VISIT MARY\n",
      "   67: *LIKE *MOTHER NOT BUY HOUSE                                   JOHN FUTURE NOT BUY HOUSE\n",
      "   71: JOHN *FINISH *GIVE1 MARY                                      JOHN WILL VISIT MARY\n",
      "   74: *GO *WHO *GO *GO                                              JOHN NOT VISIT MARY\n",
      "   77: *IX BLAME *LOVE                                               ANN BLAME MARY\n",
      "   84: *HOMEWORK *GIVE1 *POSS BOOK                                   IX-1P FIND SOMETHING-ONE BOOK\n",
      "   89: *MAN *GIVE *WOMAN *IX IX *BUY *BOOK                           JOHN IX GIVE MAN IX NEW COAT\n",
      "   90: JOHN *GIVE1 IX *GIVE3 *GIVE1 *COAT                            JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "   92: JOHN *WOMAN *WOMAN *WOMAN WOMAN BOOK                          JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "  100: POSS NEW CAR BREAK-DOWN                                       POSS NEW CAR BREAK-DOWN\n",
      "  105: *FRANK *VEGETABLE                                             JOHN LEG\n",
      "  107: *LIKE *SOMETHING-ONE *HAVE *GO *WHO                           JOHN POSS FRIEND HAVE CANDY\n",
      "  108: *IX ARRIVE                                                    WOMAN ARRIVE\n",
      "  113: IX CAR *SUE *SOMETHING-ONE *ARRIVE                            IX CAR BLUE SUE BUY\n",
      "  119: *PREFER *BUY1 IX CAR *SOMETHING-ONE                           SUE BUY IX CAR BLUE\n",
      "  122: JOHN *GIVE1 BOOK                                              JOHN READ BOOK\n",
      "  139: *SHOULD *BUY1 *CAR YESTERDAY BOOK                             JOHN BUY WHAT YESTERDAY BOOK\n",
      "  142: *FRANK BUY YESTERDAY WHAT BOOK                                JOHN BUY YESTERDAY WHAT BOOK\n",
      "  158: LOVE *MARY WHO                                                LOVE JOHN WHO\n",
      "  167: *MARY *SOMETHING-ONE *MARY LOVE *LOVE                         JOHN IX SAY LOVE MARY\n",
      "  171: *SOMETHING-ONE *SOMETHING-ONE BLAME                           JOHN MARY BLAME\n",
      "  174: *CAN *GIVE3 GIVE1 *GO *WHAT                                   PEOPLE GROUP GIVE1 JANA TOY\n",
      "  181: *SUE ARRIVE                                                   JOHN ARRIVE\n",
      "  184: *IX BOY *GIVE1 TEACHER APPLE                                  ALL BOY GIVE TEACHER APPLE\n",
      "  189: *SUE *SOMETHING-ONE *YESTERDAY *ARRIVE                        JOHN GIVE GIRL BOX\n",
      "  193: JOHN *SOMETHING-ONE *YESTERDAY BOX                            JOHN GIVE GIRL BOX\n",
      "  199: *HOMEWORK CHOCOLATE WHO                                       LIKE CHOCOLATE WHO\n",
      "  201: JOHN *MAN *MAN *JOHN BUY HOUSE                                JOHN TELL MARY IX-1P BUY HOUSE\n"
     ]
    }
   ],
   "source": [
    "# Choose a feature set and model selector\n",
    "features = features_polar # change as needed\n",
    "model_selector = SelectorCV # change as needed\n",
    "\n",
    "# Recognize the test set and display the result with the show_errors method\n",
    "models = train_all_words(features, model_selector)\n",
    "test_set = asl.build_test(features)\n",
    "probabilities, guesses = recognize(models, test_set)\n",
    "show_errors(guesses, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** WER = 0.5337078651685393\n",
      "Total correct: 83 out of 178\n",
      "Video  Recognized                                                    Correct\n",
      "=====================================================================================================\n",
      "    2: *IX WRITE *ARRIVE                                             JOHN WRITE HOMEWORK\n",
      "    7: JOHN *PEOPLE *VISIT *ARRIVE                                   JOHN CAN GO CAN\n",
      "   12: *IX CAN *WHAT CAN                                             JOHN CAN GO CAN\n",
      "   21: *IX *NEW *SOMETHING-ONE *MARY *CAR *CAR *FUTURE *MARY         JOHN FISH WONT EAT BUT CAN EAT CHICKEN\n",
      "   25: *IX *MARY IX IX IX                                            JOHN LIKE IX IX IX\n",
      "   28: *IX *MARY IX IX IX                                            JOHN LIKE IX IX IX\n",
      "   30: *IX LIKE *MARY *LIKE IX                                       JOHN LIKE IX IX IX\n",
      "   36: MARY *JOHN *IX IX *MARY *MARY                                 MARY VEGETABLE KNOW IX LIKE CORN1\n",
      "   40: JOHN IX *CORN MARY *MARY                                      JOHN IX THINK MARY LOVE\n",
      "   43: *IX *IX BUY HOUSE                                             JOHN MUST BUY HOUSE\n",
      "   50: *POSS *POSS BUY CAR *IX                                       FUTURE JOHN BUY CAR SHOULD\n",
      "   54: JOHN SHOULD *WHO BUY HOUSE                                    JOHN SHOULD NOT BUY HOUSE\n",
      "   57: JOHN *JOHN *IX MARY                                           JOHN DECIDE VISIT MARY\n",
      "   67: *FRANK *IX *MARY BUY HOUSE                                    JOHN FUTURE NOT BUY HOUSE\n",
      "   71: *IX *IX *BLAME MARY                                           JOHN WILL VISIT MARY\n",
      "   74: *IX *MARY *MARY MARY                                          JOHN NOT VISIT MARY\n",
      "   77: *IX BLAME MARY                                                ANN BLAME MARY\n",
      "   84: *IX *ARRIVE SOMETHING-ONE BOOK                                IX-1P FIND SOMETHING-ONE BOOK\n",
      "   89: *SOMETHING-ONE IX *IX *IX IX NEW COAT                         JOHN IX GIVE MAN IX NEW COAT\n",
      "   90: JOHN *IX IX *IX *MARY BOOK                                    JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "   92: *IX *IX *SOMETHING-ONE *WOMAN WOMAN BOOK                      JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "  100: POSS NEW CAR BREAK-DOWN                                       POSS NEW CAR BREAK-DOWN\n",
      "  105: *FRANK *POSS                                                  JOHN LEG\n",
      "  107: JOHN *IX *HAVE HAVE *JOHN                                     JOHN POSS FRIEND HAVE CANDY\n",
      "  108: *MARY *HOMEWORK                                               WOMAN ARRIVE\n",
      "  113: IX CAR *IX *JOHN *ARRIVE                                      IX CAR BLUE SUE BUY\n",
      "  119: *MARY *BUY1 IX CAR *JOHN                                      SUE BUY IX CAR BLUE\n",
      "  122: JOHN *GIVE1 BOOK                                              JOHN READ BOOK\n",
      "  139: JOHN *BUY1 WHAT YESTERDAY BOOK                                JOHN BUY WHAT YESTERDAY BOOK\n",
      "  142: *FRANK BUY YESTERDAY WHAT BOOK                                JOHN BUY YESTERDAY WHAT BOOK\n",
      "  158: LOVE *MARY WHO                                                LOVE JOHN WHO\n",
      "  167: JOHN IX *MARY LOVE *LOVE                                      JOHN IX SAY LOVE MARY\n",
      "  171: *MARY *JOHN BLAME                                             JOHN MARY BLAME\n",
      "  174: *ARRIVE *ARRIVE GIVE1 *JOHN *BLAME                            PEOPLE GROUP GIVE1 JANA TOY\n",
      "  181: JOHN ARRIVE                                                   JOHN ARRIVE\n",
      "  184: *IX BOY *GIVE1 TEACHER APPLE                                  ALL BOY GIVE TEACHER APPLE\n",
      "  189: JOHN *IX *JOHN *ARRIVE                                        JOHN GIVE GIRL BOX\n",
      "  193: JOHN *IX *IX BOX                                              JOHN GIVE GIRL BOX\n",
      "  199: *IX CHOCOLATE *MARY                                           LIKE CHOCOLATE WHO\n",
      "  201: JOHN *MAN *IX *WOMAN BUY HOUSE                                JOHN TELL MARY IX-1P BUY HOUSE\n"
     ]
    }
   ],
   "source": [
    "# Choose a feature set and model selector\n",
    "features = np.concatenate((features_norm, features_polar, features_delta), axis=0) # change as needed\n",
    "model_selector = SelectorCV # change as needed\n",
    "\n",
    "# Recognize the test set and display the result with the show_errors method\n",
    "models = train_all_words(features, model_selector)\n",
    "test_set = asl.build_test(features)\n",
    "probabilities, guesses = recognize(models, test_set)\n",
    "show_errors(guesses, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** WER = 0.5056179775280899\n",
      "Total correct: 88 out of 178\n",
      "Video  Recognized                                                    Correct\n",
      "=====================================================================================================\n",
      "    2: JOHN WRITE HOMEWORK                                           JOHN WRITE HOMEWORK\n",
      "    7: JOHN *PEOPLE GO *ARRIVE                                       JOHN CAN GO CAN\n",
      "   12: JOHN CAN *GO1 CAN                                             JOHN CAN GO CAN\n",
      "   21: JOHN *VIDEOTAPE *GO EAT *CAR *CAR *FUTURE *EAT                JOHN FISH WONT EAT BUT CAN EAT CHICKEN\n",
      "   25: JOHN LIKE *LOVE *LIKE *LOVE                                   JOHN LIKE IX IX IX\n",
      "   28: JOHN LIKE *LOVE *JOHN *LOVE                                   JOHN LIKE IX IX IX\n",
      "   30: JOHN LIKE *LOVE *JOHN IX                                      JOHN LIKE IX IX IX\n",
      "   36: MARY *WHO *IX IX *MARY *JOHN                                  MARY VEGETABLE KNOW IX LIKE CORN1\n",
      "   40: JOHN IX *SUE *WHO *MARY                                       JOHN IX THINK MARY LOVE\n",
      "   43: JOHN *JOHN BUY HOUSE                                          JOHN MUST BUY HOUSE\n",
      "   50: *JOHN *SEE BUY CAR *JOHN                                      FUTURE JOHN BUY CAR SHOULD\n",
      "   54: JOHN *JOHN *WHO BUY HOUSE                                     JOHN SHOULD NOT BUY HOUSE\n",
      "   57: JOHN *WHO *IX *IX                                             JOHN DECIDE VISIT MARY\n",
      "   67: JOHN FUTURE *JOHN BUY HOUSE                                   JOHN FUTURE NOT BUY HOUSE\n",
      "   71: JOHN *JOHN VISIT MARY                                         JOHN WILL VISIT MARY\n",
      "   74: JOHN *MARY *IX *IX                                            JOHN NOT VISIT MARY\n",
      "   77: *JOHN BLAME *LOVE                                             ANN BLAME MARY\n",
      "   84: *JOHN *ARRIVE *VISIT *NEW-YORK                                IX-1P FIND SOMETHING-ONE BOOK\n",
      "   89: JOHN IX *IX *IX IX NEW COAT                                   JOHN IX GIVE MAN IX NEW COAT\n",
      "   90: *PREFER *IX IX *IX WOMAN BOOK                                 JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "   92: JOHN *IX IX *IX *IX BOOK                                      JOHN GIVE IX SOMETHING-ONE WOMAN BOOK\n",
      "  100: *IX NEW CAR BREAK-DOWN                                        POSS NEW CAR BREAK-DOWN\n",
      "  105: JOHN *TELL                                                    JOHN LEG\n",
      "  107: JOHN POSS *ARRIVE *MARY *MARY                                 JOHN POSS FRIEND HAVE CANDY\n",
      "  108: *JOHN *HOMEWORK                                               WOMAN ARRIVE\n",
      "  113: *JOHN CAR *IX SUE *ARRIVE                                     IX CAR BLUE SUE BUY\n",
      "  119: *WHO *BUY1 IX CAR *FUTURE                                     SUE BUY IX CAR BLUE\n",
      "  122: JOHN *GIVE1 BOOK                                              JOHN READ BOOK\n",
      "  139: JOHN *BUY1 WHAT *WHAT BOOK                                    JOHN BUY WHAT YESTERDAY BOOK\n",
      "  142: JOHN BUY YESTERDAY WHAT *NEW-YORK                             JOHN BUY YESTERDAY WHAT BOOK\n",
      "  158: LOVE *WHO WHO                                                 LOVE JOHN WHO\n",
      "  167: JOHN IX *IX LOVE *IX                                          JOHN IX SAY LOVE MARY\n",
      "  171: JOHN *JOHN BLAME                                              JOHN MARY BLAME\n",
      "  174: *CAR *GIVE1 GIVE1 *WHO *CAR                                   PEOPLE GROUP GIVE1 JANA TOY\n",
      "  181: *VISIT ARRIVE                                                 JOHN ARRIVE\n",
      "  184: *IX *FUTURE *GIVE1 TEACHER *WHO                               ALL BOY GIVE TEACHER APPLE\n",
      "  189: JOHN *IX *WHO *ARRIVE                                         JOHN GIVE GIRL BOX\n",
      "  193: JOHN *IX *IX BOX                                              JOHN GIVE GIRL BOX\n",
      "  199: *JOHN CHOCOLATE WHO                                           LIKE CHOCOLATE WHO\n",
      "  201: JOHN *THINK MARY *WOMAN BUY HOUSE                             JOHN TELL MARY IX-1P BUY HOUSE\n"
     ]
    }
   ],
   "source": [
    "# Choose a feature set and model selector\n",
    "features = np.concatenate((features_norm, features_delta), axis=0) # change as needed\n",
    "model_selector = SelectorCV # change as needed\n",
    "\n",
    "# Recognize the test set and display the result with the show_errors method\n",
    "models = train_all_words(features, model_selector)\n",
    "test_set = asl.build_test(features)\n",
    "probabilities, guesses = recognize(models, test_set)\n",
    "show_errors(guesses, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Question 3:**  Summarize the error results from three combinations of features and model selectors.  What was the \"best\" combination and why?  What additional information might we use to improve our WER?  For more insight on improving WER, take a look at the introduction to Part 4.\n",
    "\n",
    "**Answer 3:** In all test runs, SelectorCV performed at or better than the rest of the selectors. This is indicative of a small training set. The cross-fold simulates a larger dataset and produces a lower WER. The best combination included features_norm and features_delta with the SelectorCV. By combining the engineered feature sets, the model was able to get more insight and distingtion into the different nuances in the words. To improve WER, we could take into account the frequency words are typically used. Or we could use features based on the context of the sentence with which they take place. For example, the word 'to' is commonly used in English and therefore has a high probability to occur in a sentence. Additionally, some words have a high liklihood of following other words while some words have almost no probability. For example, 'go to' is often seen together, but, you are very unlikely to see the combination 'go chocolate'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part3_test'></a>\n",
    "### Recognizer Unit Tests\n",
    "Run the following unit tests as a sanity check on the defined recognizer.  The test simply looks for some valid values but is not exhaustive. However, the project should not be submitted if these tests don't pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 33.340s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.runner.TextTestResult run=2 errors=0 failures=0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from asl_test_recognizer import TestRecognize\n",
    "suite = unittest.TestLoader().loadTestsFromModule(TestRecognize())\n",
    "unittest.TextTestRunner().run(suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<a id='part4_info'></a>\n",
    "## PART 4: (OPTIONAL)  Improve the WER with Language Models\n",
    "We've squeezed just about as much as we can out of the model and still only get about 50% of the words right! Surely we can do better than that.  Probability to the rescue again in the form of [statistical language models (SLM)](https://en.wikipedia.org/wiki/Language_model).  The basic idea is that each word has some probability of occurrence within the set, and some probability that it is adjacent to specific other words. We can use that additional information to make better choices.\n",
    "\n",
    "##### Additional reading and resources\n",
    "- [Introduction to N-grams (Stanford Jurafsky slides)](https://web.stanford.edu/class/cs124/lec/languagemodeling.pdf)\n",
    "- [Speech Recognition Techniques for a Sign Language Recognition System, Philippe Dreuw et al](https://www-i6.informatik.rwth-aachen.de/publications/download/154/Dreuw--2007.pdf) see the improved results of applying LM on *this* data!\n",
    "- [SLM data for *this* ASL dataset](ftp://wasserstoff.informatik.rwth-aachen.de/pub/rwth-boston-104/lm/)\n",
    "\n",
    "##### Optional challenge\n",
    "The recognizer you implemented in Part 3 is equivalent to a \"0-gram\" SLM.  Improve the WER with the SLM data provided with the data set in the link above using \"1-gram\", \"2-gram\", and/or \"3-gram\" statistics. The `probabilities` data you've already calculated will be useful and can be turned into a pandas DataFrame if desired (see next cell).  \n",
    "Good luck!  Share your results with the class!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ALL</th>\n",
       "      <th>ANN</th>\n",
       "      <th>APPLE</th>\n",
       "      <th>ARRIVE</th>\n",
       "      <th>BILL</th>\n",
       "      <th>BLAME</th>\n",
       "      <th>BLUE</th>\n",
       "      <th>BOOK</th>\n",
       "      <th>BORROW</th>\n",
       "      <th>BOX</th>\n",
       "      <th>...</th>\n",
       "      <th>VIDEOTAPE</th>\n",
       "      <th>VISIT</th>\n",
       "      <th>WANT</th>\n",
       "      <th>WHAT</th>\n",
       "      <th>WHO</th>\n",
       "      <th>WILL</th>\n",
       "      <th>WOMAN</th>\n",
       "      <th>WONT</th>\n",
       "      <th>WRITE</th>\n",
       "      <th>YESTERDAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-853.551803</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1719.275289</td>\n",
       "      <td>-348.606697</td>\n",
       "      <td>-187274.038624</td>\n",
       "      <td>-351.812833</td>\n",
       "      <td>-1355.408515</td>\n",
       "      <td>-644.512718</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-494.393292</td>\n",
       "      <td>...</td>\n",
       "      <td>-835.924269</td>\n",
       "      <td>-121.007482</td>\n",
       "      <td>-1942.561230</td>\n",
       "      <td>-309.662467</td>\n",
       "      <td>-127.048273</td>\n",
       "      <td>-14048.723331</td>\n",
       "      <td>-457.877260</td>\n",
       "      <td>-491.309529</td>\n",
       "      <td>-402.874541</td>\n",
       "      <td>-223.553666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3164.645264</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-5183.260896</td>\n",
       "      <td>-189.782924</td>\n",
       "      <td>-145147.095698</td>\n",
       "      <td>-284.367866</td>\n",
       "      <td>-2189.257131</td>\n",
       "      <td>-207.587495</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-278.388070</td>\n",
       "      <td>...</td>\n",
       "      <td>-219.993440</td>\n",
       "      <td>-204.544518</td>\n",
       "      <td>-2589.010078</td>\n",
       "      <td>-251.808932</td>\n",
       "      <td>-253.976315</td>\n",
       "      <td>-15829.241752</td>\n",
       "      <td>-546.193450</td>\n",
       "      <td>-925.380186</td>\n",
       "      <td>-142.614661</td>\n",
       "      <td>-503.218598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3784.150973</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-6953.739171</td>\n",
       "      <td>-398.918978</td>\n",
       "      <td>-251975.815440</td>\n",
       "      <td>-525.896933</td>\n",
       "      <td>-4888.877160</td>\n",
       "      <td>-523.769416</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-550.875056</td>\n",
       "      <td>...</td>\n",
       "      <td>-518.233254</td>\n",
       "      <td>-424.542029</td>\n",
       "      <td>-4917.492924</td>\n",
       "      <td>-568.671255</td>\n",
       "      <td>-449.674498</td>\n",
       "      <td>-19218.662446</td>\n",
       "      <td>-1018.986983</td>\n",
       "      <td>-1386.703778</td>\n",
       "      <td>-724.513626</td>\n",
       "      <td>-787.620982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-683.248076</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1160.454981</td>\n",
       "      <td>-370.950075</td>\n",
       "      <td>-21608.131754</td>\n",
       "      <td>-476.073850</td>\n",
       "      <td>-1017.874594</td>\n",
       "      <td>-783.199643</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-608.330271</td>\n",
       "      <td>...</td>\n",
       "      <td>-1032.979302</td>\n",
       "      <td>-154.101190</td>\n",
       "      <td>-1458.098928</td>\n",
       "      <td>-275.848949</td>\n",
       "      <td>-134.471679</td>\n",
       "      <td>-2483.962610</td>\n",
       "      <td>-343.369900</td>\n",
       "      <td>-405.993916</td>\n",
       "      <td>-532.139339</td>\n",
       "      <td>-302.177345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1462.075479</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-1469.769551</td>\n",
       "      <td>-102.400224</td>\n",
       "      <td>-575359.556620</td>\n",
       "      <td>-110.930093</td>\n",
       "      <td>-373.430449</td>\n",
       "      <td>-139.481675</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-135.687187</td>\n",
       "      <td>...</td>\n",
       "      <td>-192.266865</td>\n",
       "      <td>-94.524881</td>\n",
       "      <td>-386.138518</td>\n",
       "      <td>-145.654983</td>\n",
       "      <td>-146.268614</td>\n",
       "      <td>-12708.918941</td>\n",
       "      <td>-436.640435</td>\n",
       "      <td>-315.363485</td>\n",
       "      <td>-190.252354</td>\n",
       "      <td>-274.831989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  112 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ALL  ANN        APPLE      ARRIVE           BILL       BLAME  \\\n",
       "0  -853.551803 -inf -1719.275289 -348.606697 -187274.038624 -351.812833   \n",
       "1 -3164.645264 -inf -5183.260896 -189.782924 -145147.095698 -284.367866   \n",
       "2 -3784.150973 -inf -6953.739171 -398.918978 -251975.815440 -525.896933   \n",
       "3  -683.248076 -inf -1160.454981 -370.950075  -21608.131754 -476.073850   \n",
       "4 -1462.075479 -inf -1469.769551 -102.400224 -575359.556620 -110.930093   \n",
       "\n",
       "          BLUE        BOOK  BORROW         BOX     ...        VIDEOTAPE  \\\n",
       "0 -1355.408515 -644.512718    -inf -494.393292     ...      -835.924269   \n",
       "1 -2189.257131 -207.587495    -inf -278.388070     ...      -219.993440   \n",
       "2 -4888.877160 -523.769416    -inf -550.875056     ...      -518.233254   \n",
       "3 -1017.874594 -783.199643    -inf -608.330271     ...     -1032.979302   \n",
       "4  -373.430449 -139.481675    -inf -135.687187     ...      -192.266865   \n",
       "\n",
       "        VISIT         WANT        WHAT         WHO          WILL        WOMAN  \\\n",
       "0 -121.007482 -1942.561230 -309.662467 -127.048273 -14048.723331  -457.877260   \n",
       "1 -204.544518 -2589.010078 -251.808932 -253.976315 -15829.241752  -546.193450   \n",
       "2 -424.542029 -4917.492924 -568.671255 -449.674498 -19218.662446 -1018.986983   \n",
       "3 -154.101190 -1458.098928 -275.848949 -134.471679  -2483.962610  -343.369900   \n",
       "4  -94.524881  -386.138518 -145.654983 -146.268614 -12708.918941  -436.640435   \n",
       "\n",
       "          WONT       WRITE   YESTERDAY  \n",
       "0  -491.309529 -402.874541 -223.553666  \n",
       "1  -925.380186 -142.614661 -503.218598  \n",
       "2 -1386.703778 -724.513626 -787.620982  \n",
       "3  -405.993916 -532.139339 -302.177345  \n",
       "4  -315.363485 -190.252354 -274.831989  \n",
       "\n",
       "[5 rows x 112 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a DataFrame of log likelihoods for the test word items\n",
    "df_probs = pd.DataFrame(data=probabilities)\n",
    "df_probs.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "nbpresent": {
   "slides": {
    "0a2d4faf-9fb8-4cee-853b-ed68b90f3f8a": {
     "id": "0a2d4faf-9fb8-4cee-853b-ed68b90f3f8a",
     "prev": null,
     "regions": {
      "3fb9ce83-fbb2-4995-832a-f8f400734ad3": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "1dbb9346-179b-4835-b430-6369d88f1a1b",
        "part": "whole"
       },
       "id": "3fb9ce83-fbb2-4995-832a-f8f400734ad3"
      }
     }
    },
    "1519a4fa-1588-4644-98de-9c43bf0aceb5": {
     "id": "1519a4fa-1588-4644-98de-9c43bf0aceb5",
     "prev": "8a712017-49b7-449f-8264-43a032ace902",
     "regions": {
      "29546121-ed11-44b7-8144-0c44e874098f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "365590a4-6963-4812-a1cf-688f7b6bb9ff",
        "part": "whole"
       },
       "id": "29546121-ed11-44b7-8144-0c44e874098f"
      }
     }
    },
    "176eaccb-15dd-455d-bf07-504213e7aa01": {
     "id": "176eaccb-15dd-455d-bf07-504213e7aa01",
     "prev": "de6b30f4-2463-4901-92ed-aabad78e5e0f",
     "regions": {
      "1542aa9e-dc55-4b90-adef-bf5181872b42": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "5c242050-c1f7-4b3b-8103-2ea9d71a40dc",
        "part": "whole"
       },
       "id": "1542aa9e-dc55-4b90-adef-bf5181872b42"
      }
     }
    },
    "19091b36-b0e7-49b1-b501-ec05937e0da9": {
     "id": "19091b36-b0e7-49b1-b501-ec05937e0da9",
     "prev": "1983c02e-fb99-4c05-a728-e0c0ad7c06d8",
     "regions": {
      "6529a31c-8d45-425c-b1d7-d0ac6fca6a32": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e766909d-9421-4aaf-9fb1-bc90d27e49e3",
        "part": "whole"
       },
       "id": "6529a31c-8d45-425c-b1d7-d0ac6fca6a32"
      }
     }
    },
    "1983c02e-fb99-4c05-a728-e0c0ad7c06d8": {
     "id": "1983c02e-fb99-4c05-a728-e0c0ad7c06d8",
     "prev": "176eaccb-15dd-455d-bf07-504213e7aa01",
     "regions": {
      "1c4e605d-7f22-4f30-b3fb-74b2937e7a4a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "4d217204-e5c0-4568-bd30-12c2e41b681d",
        "part": "whole"
       },
       "id": "1c4e605d-7f22-4f30-b3fb-74b2937e7a4a"
      }
     }
    },
    "212b111f-4527-459c-8297-1db5580ee5c9": {
     "id": "212b111f-4527-459c-8297-1db5580ee5c9",
     "prev": "76898529-e49e-4663-8d02-8261dfe1d94b",
     "regions": {
      "2e4bd280-3cd6-47d0-9c81-17737b24053b": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "0c316996-9933-4b3d-82ec-259518dc8bc9",
        "part": "whole"
       },
       "id": "2e4bd280-3cd6-47d0-9c81-17737b24053b"
      }
     }
    },
    "23a7337f-a0cf-4ed4-baa9-ec06bfdc0579": {
     "id": "23a7337f-a0cf-4ed4-baa9-ec06bfdc0579",
     "prev": "e76e9a02-54c1-4ec9-80fb-c611ed398122",
     "regions": {
      "b5721d20-d6f8-4ddb-a5aa-eb16f0cc8893": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "313015a2-b5a9-4136-a8ea-5d011e47d840",
        "part": "whole"
       },
       "id": "b5721d20-d6f8-4ddb-a5aa-eb16f0cc8893"
      }
     }
    },
    "732f1952-ee54-46fb-8067-099512824296": {
     "id": "732f1952-ee54-46fb-8067-099512824296",
     "prev": "0a2d4faf-9fb8-4cee-853b-ed68b90f3f8a",
     "regions": {
      "f31d4597-08ad-4c46-ad52-4bd2d775c624": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "aadfec52-27ca-4541-8920-fa9253d51827",
        "part": "whole"
       },
       "id": "f31d4597-08ad-4c46-ad52-4bd2d775c624"
      }
     }
    },
    "76898529-e49e-4663-8d02-8261dfe1d94b": {
     "id": "76898529-e49e-4663-8d02-8261dfe1d94b",
     "prev": "19091b36-b0e7-49b1-b501-ec05937e0da9",
     "regions": {
      "ec1746fc-aec9-4a7c-8225-9e9ac8d45889": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b3e539be-84e2-49ce-a183-31cfc5c7ce7c",
        "part": "whole"
       },
       "id": "ec1746fc-aec9-4a7c-8225-9e9ac8d45889"
      }
     }
    },
    "8a712017-49b7-449f-8264-43a032ace902": {
     "id": "8a712017-49b7-449f-8264-43a032ace902",
     "prev": "bed9e696-630e-4747-be1c-bc3737ba992f",
     "regions": {
      "1faab517-cd16-4c63-bb01-a67246749d7a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "3f14ddf0-4145-4687-9c33-712c3c32520f",
        "part": "whole"
       },
       "id": "1faab517-cd16-4c63-bb01-a67246749d7a"
      }
     }
    },
    "90af992d-eb6d-4496-b2d2-6aa9a95b6a61": {
     "id": "90af992d-eb6d-4496-b2d2-6aa9a95b6a61",
     "prev": "732f1952-ee54-46fb-8067-099512824296",
     "regions": {
      "4f448bec-5be9-4553-88ae-e35ed7612f25": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c445fbfb-b8ab-4e9a-8d13-12231a1c588f",
        "part": "whole"
       },
       "id": "4f448bec-5be9-4553-88ae-e35ed7612f25"
      }
     }
    },
    "bed9e696-630e-4747-be1c-bc3737ba992f": {
     "id": "bed9e696-630e-4747-be1c-bc3737ba992f",
     "prev": "23a7337f-a0cf-4ed4-baa9-ec06bfdc0579",
     "regions": {
      "ac1513f0-404f-492b-8b42-0313e9a753b0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "18dd2eee-8b6c-4a5e-9539-132d00a7c7e1",
        "part": "whole"
       },
       "id": "ac1513f0-404f-492b-8b42-0313e9a753b0"
      }
     }
    },
    "de6b30f4-2463-4901-92ed-aabad78e5e0f": {
     "id": "de6b30f4-2463-4901-92ed-aabad78e5e0f",
     "prev": "e36b4639-be8c-46f7-a8c9-bcfb134f9fd0",
     "regions": {
      "55ec36e0-362f-4fd3-8060-7cee056039aa": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c3cf461e-4c9e-4dec-99d2-07bfa79cbe23",
        "part": "whole"
       },
       "id": "55ec36e0-362f-4fd3-8060-7cee056039aa"
      }
     }
    },
    "e36b4639-be8c-46f7-a8c9-bcfb134f9fd0": {
     "id": "e36b4639-be8c-46f7-a8c9-bcfb134f9fd0",
     "prev": "1519a4fa-1588-4644-98de-9c43bf0aceb5",
     "regions": {
      "4c1e9714-9ba0-45fd-8a2f-ef80a5c85c2e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6534d4dc-125f-47e6-a022-cf1e0d277174",
        "part": "whole"
       },
       "id": "4c1e9714-9ba0-45fd-8a2f-ef80a5c85c2e"
      }
     }
    },
    "e76e9a02-54c1-4ec9-80fb-c611ed398122": {
     "id": "e76e9a02-54c1-4ec9-80fb-c611ed398122",
     "prev": "90af992d-eb6d-4496-b2d2-6aa9a95b6a61",
     "regions": {
      "9491b84d-193b-40ff-9321-d21eb1ba88d4": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "b64ec10e-fa9d-4f3f-907f-6799611ed6b1",
        "part": "whole"
       },
       "id": "9491b84d-193b-40ff-9321-d21eb1ba88d4"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
